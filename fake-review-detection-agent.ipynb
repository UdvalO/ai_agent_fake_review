{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30918,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"**Author:** Udval Oyunsaikhan  \n**Date:** 20 Apr, 2025  \n**Contact:** [LinkedIn](https://www.linkedin.com/in/o-udval/)  \n**Submission Video:** [Youtube](https://youtu.be/SGshgry_PQA)\n\n<img src=\"https://raw.githubusercontent.com/UdvalO/ai_agent_fake_review/main/poster.png\" alt=\"Poster\" width=\"1000\"/>","metadata":{}},{"cell_type":"markdown","source":"# Fake Review Detection Agent\nThis agent is designed to automatically detect potentially fake online reviews by analyzing both their content similarity to known fake/real examples and their writing style. It uses a k-Nearest Neighbors search on review embeddings stored in ChromaDB for similarity analysis and a Gemini LLM to assess linguistic patterns indicative of AI generation. The LangGraph workflow orchestrates these distinct analysis steps in a defined sequence, combining their outputs to produce a final prediction (Fake/Real) with a supporting explanation.\n\nSo, in building this fake review detector, I really leaned into a few key GenAI techniques that I learned from the course:\n- **Embeddings:** I used Gemini's text-embedding-004 model to turn the text of each review into a numerical vector, capturing its semantic meaning. This was crucial for comparing reviews.\n- **Vector Database & Search:** I stored these review embeddings in ChromaDB. This allowed me to quickly search for and find the most similar training reviews (k-Nearest Neighbors) when analyzing a new one.\n- **Structured Output (via Prompting):** For the stylistic analysis, I carefully prompted the Gemini LLM to classify the writing as 'Bot' or 'Human' and provide an explanation, specifically requesting it return the results in a consistent format (\"Classification: ...\", \"Explanation: ...\"). I then parsed this structured text in my code.\n- **Agents (with LangGraph):** Finally, I used LangGraph to tie everything together. I built an agent workflow where the state (the review and analysis results) automatically flows through distinct steps: first the k-NN check, then the LLM style analysis, and finally a node to combine these signals into the final Fake/Real prediction.\n\nðŸ“Œ Check out the explanation [video](https://youtu.be/SGshgry_PQA) where I walk through the code step by step, with visuals and reasoning!ðŸŽ¥","metadata":{}},{"cell_type":"markdown","source":"## Implementation\n### 1. Set-up\n\nFirst things first, gotta get the tools ready! We need google-genai to talk to the Gemini models (for both embeddings and the text analysis), chromadb to act as our vector database for finding similar reviews, and datasets to easily load our training/testing data. Later on, you see installs specifically for langgraph which we use to structure the different steps of our detection process into a neat workflow or \"agent\". Ideally we would like to work with only google-genai, but I was running into error in the embedding section and needed to work with google-generativeai.","metadata":{}},{"cell_type":"code","source":"!pip install -qU \"google-genai==1.7.0\" \"chromadb==0.6.3\"\n!pip install datasets\n!pip install --upgrade google-generativeai google-api-core\n!pip install chromadb google-generativeai","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-18T09:03:46.142028Z","iopub.execute_input":"2025-04-18T09:03:46.142502Z","iopub.status.idle":"2025-04-18T09:04:05.860226Z","shell.execute_reply.started":"2025-04-18T09:03:46.142462Z","shell.execute_reply":"2025-04-18T09:04:05.859023Z"},"scrolled":true,"_kg_hide-output":true},"outputs":[{"name":"stdout","text":"Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (3.3.1)\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.17.0)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.26.4)\nRequirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (19.0.1)\nRequirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.8)\nRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.2.3)\nRequirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.32.3)\nRequirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.67.1)\nRequirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.5.0)\nRequirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.16)\nRequirement already satisfied: fsspec<=2024.12.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets) (2024.12.0)\nRequirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.11.12)\nRequirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.29.0)\nRequirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (24.2)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.2)\nRequirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.4.6)\nRequirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.2)\nRequirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (5.0.1)\nRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (25.1.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.5.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.1.0)\nRequirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (0.2.1)\nRequirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.18.3)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.24.0->datasets) (4.12.2)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->datasets) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->datasets) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->datasets) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->datasets) (2025.0.1)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->datasets) (2022.0.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->datasets) (2.4.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.4.1)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2.3.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2025.1.31)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2025.1)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2025.1)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\nRequirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.17->datasets) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.17->datasets) (2022.0.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->datasets) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy>=1.17->datasets) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy>=1.17->datasets) (2024.2.0)\nRequirement already satisfied: google-generativeai in /usr/local/lib/python3.10/dist-packages (0.8.5)\nRequirement already satisfied: google-api-core in /usr/local/lib/python3.10/dist-packages (2.24.2)\nRequirement already satisfied: google-ai-generativelanguage==0.6.15 in /usr/local/lib/python3.10/dist-packages (from google-generativeai) (0.6.15)\nRequirement already satisfied: google-api-python-client in /usr/local/lib/python3.10/dist-packages (from google-generativeai) (2.155.0)\nRequirement already satisfied: google-auth>=2.15.0 in /usr/local/lib/python3.10/dist-packages (from google-generativeai) (2.27.0)\nRequirement already satisfied: protobuf in /usr/local/lib/python3.10/dist-packages (from google-generativeai) (5.29.4)\nRequirement already satisfied: pydantic in /usr/local/lib/python3.10/dist-packages (from google-generativeai) (2.11.0a2)\nRequirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from google-generativeai) (4.67.1)\nRequirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from google-generativeai) (4.12.2)\nRequirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in /usr/local/lib/python3.10/dist-packages (from google-ai-generativelanguage==0.6.15->google-generativeai) (1.25.0)\nRequirement already satisfied: googleapis-common-protos<2.0.0,>=1.56.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core) (1.66.0)\nRequirement already satisfied: requests<3.0.0,>=2.18.0 in /usr/local/lib/python3.10/dist-packages (from google-api-core) (2.32.3)\nRequirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth>=2.15.0->google-generativeai) (5.5.0)\nRequirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth>=2.15.0->google-generativeai) (0.4.1)\nRequirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth>=2.15.0->google-generativeai) (4.9)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.18.0->google-api-core) (3.4.1)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.18.0->google-api-core) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.18.0->google-api-core) (2.3.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.18.0->google-api-core) (2025.1.31)\nRequirement already satisfied: httplib2<1.dev0,>=0.19.0 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client->google-generativeai) (0.22.0)\nRequirement already satisfied: google-auth-httplib2<1.0.0,>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client->google-generativeai) (0.2.0)\nRequirement already satisfied: uritemplate<5,>=3.0.1 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client->google-generativeai) (4.1.1)\nRequirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic->google-generativeai) (0.7.0)\nRequirement already satisfied: pydantic-core==2.29.0 in /usr/local/lib/python3.10/dist-packages (from pydantic->google-generativeai) (2.29.0)\nRequirement already satisfied: grpcio<2.0dev,>=1.33.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.15->google-generativeai) (1.68.1)\nRequirement already satisfied: grpcio-status<2.0.dev0,>=1.33.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.15->google-generativeai) (1.48.2)\nRequirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in /usr/local/lib/python3.10/dist-packages (from httplib2<1.dev0,>=0.19.0->google-api-python-client->google-generativeai) (3.2.0)\nRequirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=2.15.0->google-generativeai) (0.6.1)\nRequirement already satisfied: chromadb in /usr/local/lib/python3.10/dist-packages (0.6.3)\nRequirement already satisfied: google-generativeai in /usr/local/lib/python3.10/dist-packages (0.8.5)\nRequirement already satisfied: build>=1.0.3 in /usr/local/lib/python3.10/dist-packages (from chromadb) (1.2.2.post1)\nRequirement already satisfied: pydantic>=1.9 in /usr/local/lib/python3.10/dist-packages (from chromadb) (2.11.0a2)\nRequirement already satisfied: chroma-hnswlib==0.7.6 in /usr/local/lib/python3.10/dist-packages (from chromadb) (0.7.6)\nRequirement already satisfied: fastapi>=0.95.2 in /usr/local/lib/python3.10/dist-packages (from chromadb) (0.115.12)\nRequirement already satisfied: uvicorn>=0.18.3 in /usr/local/lib/python3.10/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.34.1)\nRequirement already satisfied: numpy>=1.22.5 in /usr/local/lib/python3.10/dist-packages (from chromadb) (1.26.4)\nRequirement already satisfied: posthog>=2.4.0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (3.25.0)\nRequirement already satisfied: typing_extensions>=4.5.0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (4.12.2)\nRequirement already satisfied: onnxruntime>=1.14.1 in /usr/local/lib/python3.10/dist-packages (from chromadb) (1.21.0)\nRequirement already satisfied: opentelemetry-api>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (1.32.1)\nRequirement already satisfied: opentelemetry-exporter-otlp-proto-grpc>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (1.32.1)\nRequirement already satisfied: opentelemetry-instrumentation-fastapi>=0.41b0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (0.53b1)\nRequirement already satisfied: opentelemetry-sdk>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (1.32.1)\nRequirement already satisfied: tokenizers>=0.13.2 in /usr/local/lib/python3.10/dist-packages (from chromadb) (0.21.0)\nRequirement already satisfied: pypika>=0.48.9 in /usr/local/lib/python3.10/dist-packages (from chromadb) (0.48.9)\nRequirement already satisfied: tqdm>=4.65.0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (4.67.1)\nRequirement already satisfied: overrides>=7.3.1 in /usr/local/lib/python3.10/dist-packages (from chromadb) (7.7.0)\nRequirement already satisfied: importlib-resources in /usr/local/lib/python3.10/dist-packages (from chromadb) (5.13.0)\nRequirement already satisfied: grpcio>=1.58.0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (1.68.1)\nRequirement already satisfied: bcrypt>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from chromadb) (4.3.0)\nRequirement already satisfied: typer>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (0.15.1)\nRequirement already satisfied: kubernetes>=28.1.0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (32.0.1)\nRequirement already satisfied: tenacity>=8.2.3 in /usr/local/lib/python3.10/dist-packages (from chromadb) (9.0.0)\nRequirement already satisfied: PyYAML>=6.0.0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (6.0.2)\nRequirement already satisfied: mmh3>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from chromadb) (5.1.0)\nRequirement already satisfied: orjson>=3.9.12 in /usr/local/lib/python3.10/dist-packages (from chromadb) (3.10.12)\nRequirement already satisfied: httpx>=0.27.0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (0.28.1)\nRequirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (13.9.4)\nRequirement already satisfied: google-ai-generativelanguage==0.6.15 in /usr/local/lib/python3.10/dist-packages (from google-generativeai) (0.6.15)\nRequirement already satisfied: google-api-core in /usr/local/lib/python3.10/dist-packages (from google-generativeai) (2.24.2)\nRequirement already satisfied: google-api-python-client in /usr/local/lib/python3.10/dist-packages (from google-generativeai) (2.155.0)\nRequirement already satisfied: google-auth>=2.15.0 in /usr/local/lib/python3.10/dist-packages (from google-generativeai) (2.27.0)\nRequirement already satisfied: protobuf in /usr/local/lib/python3.10/dist-packages (from google-generativeai) (5.29.4)\nRequirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in /usr/local/lib/python3.10/dist-packages (from google-ai-generativelanguage==0.6.15->google-generativeai) (1.25.0)\nRequirement already satisfied: packaging>=19.1 in /usr/local/lib/python3.10/dist-packages (from build>=1.0.3->chromadb) (24.2)\nRequirement already satisfied: pyproject_hooks in /usr/local/lib/python3.10/dist-packages (from build>=1.0.3->chromadb) (1.2.0)\nRequirement already satisfied: tomli>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from build>=1.0.3->chromadb) (2.2.1)\nRequirement already satisfied: starlette<0.47.0,>=0.40.0 in /usr/local/lib/python3.10/dist-packages (from fastapi>=0.95.2->chromadb) (0.46.2)\nRequirement already satisfied: googleapis-common-protos<2.0.0,>=1.56.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core->google-generativeai) (1.66.0)\nRequirement already satisfied: requests<3.0.0,>=2.18.0 in /usr/local/lib/python3.10/dist-packages (from google-api-core->google-generativeai) (2.32.3)\nRequirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth>=2.15.0->google-generativeai) (5.5.0)\nRequirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth>=2.15.0->google-generativeai) (0.4.1)\nRequirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth>=2.15.0->google-generativeai) (4.9)\nRequirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx>=0.27.0->chromadb) (4.9.0)\nRequirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx>=0.27.0->chromadb) (2025.1.31)\nRequirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx>=0.27.0->chromadb) (1.0.7)\nRequirement already satisfied: idna in /usr/local/lib/python3.10/dist-packages (from httpx>=0.27.0->chromadb) (3.10)\nRequirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx>=0.27.0->chromadb) (0.14.0)\nRequirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb) (1.17.0)\nRequirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb) (2.9.0.post0)\nRequirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb) (1.8.0)\nRequirement already satisfied: requests-oauthlib in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb) (1.3.1)\nRequirement already satisfied: oauthlib>=3.2.2 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb) (3.2.2)\nRequirement already satisfied: urllib3>=1.24.2 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb) (2.3.0)\nRequirement already satisfied: durationpy>=0.7 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb) (0.9)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy>=1.22.5->chromadb) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy>=1.22.5->chromadb) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy>=1.22.5->chromadb) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy>=1.22.5->chromadb) (2025.0.1)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy>=1.22.5->chromadb) (2022.0.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy>=1.22.5->chromadb) (2.4.1)\nRequirement already satisfied: coloredlogs in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.14.1->chromadb) (15.0.1)\nRequirement already satisfied: flatbuffers in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.14.1->chromadb) (24.3.25)\nRequirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.14.1->chromadb) (1.13.1)\nRequirement already satisfied: deprecated>=1.2.6 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-api>=1.2.0->chromadb) (1.2.15)\nRequirement already satisfied: importlib-metadata<8.7.0,>=6.0 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-api>=1.2.0->chromadb) (8.5.0)\nRequirement already satisfied: opentelemetry-exporter-otlp-proto-common==1.32.1 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.32.1)\nRequirement already satisfied: opentelemetry-proto==1.32.1 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.32.1)\nRequirement already satisfied: opentelemetry-instrumentation-asgi==0.53b1 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (0.53b1)\nRequirement already satisfied: opentelemetry-instrumentation==0.53b1 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (0.53b1)\nRequirement already satisfied: opentelemetry-semantic-conventions==0.53b1 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (0.53b1)\nRequirement already satisfied: opentelemetry-util-http==0.53b1 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (0.53b1)\nRequirement already satisfied: wrapt<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-instrumentation==0.53b1->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (1.17.0)\nRequirement already satisfied: asgiref~=3.0 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-instrumentation-asgi==0.53b1->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (3.8.1)\nRequirement already satisfied: monotonic>=1.5 in /usr/local/lib/python3.10/dist-packages (from posthog>=2.4.0->chromadb) (1.6)\nRequirement already satisfied: backoff>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from posthog>=2.4.0->chromadb) (2.2.1)\nRequirement already satisfied: distro>=1.5.0 in /usr/local/lib/python3.10/dist-packages (from posthog>=2.4.0->chromadb) (1.9.0)\nRequirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic>=1.9->chromadb) (0.7.0)\nRequirement already satisfied: pydantic-core==2.29.0 in /usr/local/lib/python3.10/dist-packages (from pydantic>=1.9->chromadb) (2.29.0)\nRequirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->chromadb) (3.0.0)\nRequirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->chromadb) (2.19.1)\nRequirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from tokenizers>=0.13.2->chromadb) (0.29.0)\nRequirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from typer>=0.9.0->chromadb) (8.1.7)\nRequirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer>=0.9.0->chromadb) (1.5.4)\nRequirement already satisfied: httptools>=0.6.3 in /usr/local/lib/python3.10/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.6.4)\nRequirement already satisfied: python-dotenv>=0.13 in /usr/local/lib/python3.10/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (1.1.0)\nRequirement already satisfied: uvloop!=0.15.0,!=0.15.1,>=0.14.0 in /usr/local/lib/python3.10/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.21.0)\nRequirement already satisfied: watchfiles>=0.13 in /usr/local/lib/python3.10/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (1.0.5)\nRequirement already satisfied: websockets>=10.4 in /usr/local/lib/python3.10/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (14.1)\nRequirement already satisfied: httplib2<1.dev0,>=0.19.0 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client->google-generativeai) (0.22.0)\nRequirement already satisfied: google-auth-httplib2<1.0.0,>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client->google-generativeai) (0.2.0)\nRequirement already satisfied: uritemplate<5,>=3.0.1 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client->google-generativeai) (4.1.1)\nRequirement already satisfied: grpcio-status<2.0.dev0,>=1.33.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.15->google-generativeai) (1.48.2)\nRequirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in /usr/local/lib/python3.10/dist-packages (from httplib2<1.dev0,>=0.19.0->google-api-python-client->google-generativeai) (3.2.0)\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb) (3.17.0)\nRequirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb) (2024.12.0)\nRequirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.10/dist-packages (from importlib-metadata<8.7.0,>=6.0->opentelemetry-api>=1.2.0->chromadb) (3.21.0)\nRequirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->chromadb) (0.1.2)\nRequirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=2.15.0->google-generativeai) (0.6.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.18.0->google-api-core->google-generativeai) (3.4.1)\nRequirement already satisfied: exceptiongroup>=1.0.2 in /usr/local/lib/python3.10/dist-packages (from anyio->httpx>=0.27.0->chromadb) (1.2.2)\nRequirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio->httpx>=0.27.0->chromadb) (1.3.1)\nRequirement already satisfied: humanfriendly>=9.1 in /usr/local/lib/python3.10/dist-packages (from coloredlogs->onnxruntime>=1.14.1->chromadb) (10.0)\nRequirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.22.5->chromadb) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.22.5->chromadb) (2022.0.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy>=1.22.5->chromadb) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy>=1.22.5->chromadb) (2024.2.0)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->onnxruntime>=1.14.1->chromadb) (1.3.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy>=1.22.5->chromadb) (2024.2.0)\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"from google import genai\nfrom google.genai import types\n\nfrom IPython.display import Markdown\n\ngenai.__version__","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-18T09:10:32.973224Z","iopub.execute_input":"2025-04-18T09:10:32.973643Z","iopub.status.idle":"2025-04-18T09:10:33.860596Z","shell.execute_reply.started":"2025-04-18T09:10:32.973610Z","shell.execute_reply":"2025-04-18T09:10:33.859527Z"},"jupyter":{"outputs_hidden":true},"collapsed":true},"outputs":[{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"'1.7.0'"},"metadata":{}}],"execution_count":3},{"cell_type":"code","source":"from kaggle_secrets import UserSecretsClient\n\nGOOGLE_API_KEY = UserSecretsClient().get_secret(\"GOOGLE_API_KEY\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-18T09:10:34.811232Z","iopub.execute_input":"2025-04-18T09:10:34.811828Z","iopub.status.idle":"2025-04-18T09:10:34.962735Z","shell.execute_reply.started":"2025-04-18T09:10:34.811787Z","shell.execute_reply":"2025-04-18T09:10:34.961592Z"},"jupyter":{"outputs_hidden":true},"collapsed":true},"outputs":[],"execution_count":4},{"cell_type":"markdown","source":"#### Automated Retry\nThis codelab sends a lot of requests, so set up an automatic retry that ensures your requests are retried when per-minute quota is reached.","metadata":{}},{"cell_type":"code","source":"from google.api_core import retry\n\nis_retriable = lambda e: (isinstance(e, genai.errors.APIError) and e.code in {429, 503})\n\nif not hasattr(genai.models.Models.generate_content, '__wrapped__'):\n  genai.models.Models.generate_content = retry.Retry(\n      predicate=is_retriable)(genai.models.Models.generate_content)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-18T09:10:36.900603Z","iopub.execute_input":"2025-04-18T09:10:36.901027Z","iopub.status.idle":"2025-04-18T09:10:36.956638Z","shell.execute_reply.started":"2025-04-18T09:10:36.900997Z","shell.execute_reply":"2025-04-18T09:10:36.955424Z"}},"outputs":[],"execution_count":5},{"cell_type":"markdown","source":"###  2. Data Loading and Preparation\nThis is where we get our hands on the data. I'm using the datasets library to pull down the Fake Reviews Dataset. To make things run faster for this demo, I'm just taking a random **10%** sample. Then, standard machine learning practice: I split that sample into a training set (to build our ChromaDB database) and a test set (to evaluate the agent later). X holds the review features (category, rating, text), and y holds the actual 'Fake' (1) or 'Real' (0) labels.\n\n#### About the Dataset\n\nThe [Fake Reviews Dataset](https://huggingface.co/datasets/theArijitDas/Fake-Reviews-Dataset) by theArijitDas is a curated collection designed for training and evaluating models in fake review detection. It comprises approximately 40,526 product reviews, evenly split between:\n - 20,000 real reviews: Authentic, human-written product reviews.\n - 20,000 fake reviews: AI-generated reviews created using language models.\n\n**Each entry includes:**\n- Category: Product category (e.g., Home_and_Kitchen).\n- Rating: Numerical rating associated with the review.\n- Text: The content of the review.\n- Label: Binary indicator where 0 denotes a real review and 1 denotes a fake review.â€‹\n","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nfrom datasets import load_dataset\nfrom sklearn.model_selection import train_test_split\n\n# Load dataset\ndataset = load_dataset(\"theArijitDas/Fake-Reviews-Dataset\")\n\n# Convert the train split to a DataFrame\ndf = pd.DataFrame(dataset[\"train\"])\n\n# Reducing the data set to 10% (4000) of original data for ease \ndf_reduced = df.sample(frac=0.1, random_state=42)\n\n# Split into train and test (80% train, 20% test)\ntrain_df, test_df = train_test_split(df_reduced, test_size=0.2, random_state=42)\n\n# Check the first few rows of each\nprint(\"Train data:\")\nprint(train_df.shape)\n\nprint(\"\\nTest data:\")\nprint(test_df.shape)\n\ny_train = train_df[\"label\"]\ny_test = test_df[\"label\"]\nX_train = train_df.drop(columns=[\"label\"])\nX_test = test_df.drop(columns=[\"label\"])\n\nX_train.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-18T09:10:38.898824Z","iopub.execute_input":"2025-04-18T09:10:38.899464Z","iopub.status.idle":"2025-04-18T09:10:44.071485Z","shell.execute_reply.started":"2025-04-18T09:10:38.899424Z","shell.execute_reply":"2025-04-18T09:10:44.070325Z"},"scrolled":true},"outputs":[{"name":"stdout","text":"Train data:\n(3242, 4)\n\nTest data:\n(811, 4)\n","output_type":"stream"},{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"                         category  rating  \\\n23072                Pet_Supplies     4.0   \n39013  Clothing_Shoes_and_Jewelry     5.0   \n27667                Kindle_Store     5.0   \n20727                Pet_Supplies     5.0   \n31070                       Books     4.0   \n\n                                                    text  \n23072  The pellets are a bit larger than I would have...  \n39013  Light Linen, fresh and comfortable. True to si...  \n27667  I read the HvZ series and it kept me engaged. ...  \n20727  Phenomenal leash and locking mechanism!!  I lo...  \n31070  I liked it . However, I did not understand why...  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>category</th>\n      <th>rating</th>\n      <th>text</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>23072</th>\n      <td>Pet_Supplies</td>\n      <td>4.0</td>\n      <td>The pellets are a bit larger than I would have...</td>\n    </tr>\n    <tr>\n      <th>39013</th>\n      <td>Clothing_Shoes_and_Jewelry</td>\n      <td>5.0</td>\n      <td>Light Linen, fresh and comfortable. True to si...</td>\n    </tr>\n    <tr>\n      <th>27667</th>\n      <td>Kindle_Store</td>\n      <td>5.0</td>\n      <td>I read the HvZ series and it kept me engaged. ...</td>\n    </tr>\n    <tr>\n      <th>20727</th>\n      <td>Pet_Supplies</td>\n      <td>5.0</td>\n      <td>Phenomenal leash and locking mechanism!!  I lo...</td>\n    </tr>\n    <tr>\n      <th>31070</th>\n      <td>Books</td>\n      <td>4.0</td>\n      <td>I liked it . However, I did not understand why...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":6},{"cell_type":"code","source":"client = genai.Client(api_key=GOOGLE_API_KEY)\n\nfor m in client.models.list():\n    if m.supported_actions and \"embedContent\" in m.supported_actions:\n        print(m.name)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-18T09:10:47.862772Z","iopub.execute_input":"2025-04-18T09:10:47.863469Z","iopub.status.idle":"2025-04-18T09:10:48.299422Z","shell.execute_reply.started":"2025-04-18T09:10:47.863433Z","shell.execute_reply":"2025-04-18T09:10:48.298050Z"}},"outputs":[{"name":"stdout","text":"models/embedding-001\nmodels/text-embedding-004\nmodels/gemini-embedding-exp-03-07\nmodels/gemini-embedding-exp\n","output_type":"stream"}],"execution_count":7},{"cell_type":"markdown","source":"###  3. Embedding Function Definition\nWe need a way to convert the text reviews into numerical representations (embeddings) so we can compare them for similarity.\nChromaDB lets us define a custom function for this. So, I created the GeminiEmbeddingFunction class. \nIt uses the google-geneai library's embed_content function (specifically with the text-embedding-004 model). \nIt's designed to work with ChromaDB and has a document_mode switch: True uses the retrieval_document \ntask type (ideal for indexing the reviews we store), and False uses retrieval_query (ideal for embedding a new review we \nwant to check). I also added some basic error handling and retry logic from google.api_core.","metadata":{}},{"cell_type":"code","source":"from chromadb import Documents, EmbeddingFunction, Embeddings\nfrom google.api_core import retry\nfrom google.api_core.exceptions import ResourceExhausted, ServiceUnavailable\nimport google.generativeai as genai\nfrom google.genai import types\n\n# Retry handler\nis_retriable = lambda e: isinstance(e, (ResourceExhausted, ServiceUnavailable))\n\nclass GeminiEmbeddingFunction(EmbeddingFunction):\n    def __init__(self):\n        self.document_mode = True\n        self.model_name = \"models/text-embedding-004\"\n\n    @retry.Retry(predicate=is_retriable)\n    def __call__(self, input: Documents) -> Embeddings:\n        task_type = \"retrieval_document\" if self.document_mode else \"retrieval_query\"\n        embeddings = []\n\n        for text in input:\n            try:\n                response = genai.embed_content(\n                    model=self.model_name,\n                    content=text,\n                    task_type=task_type\n                )\n                embeddings.append(response[\"embedding\"])\n            except Exception as e:\n                print(f\"Error embedding content: {e}\")\n                embeddings.append([0.0] * 768)\n\n        return embeddings","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-18T09:11:23.793749Z","iopub.execute_input":"2025-04-18T09:11:23.794220Z","iopub.status.idle":"2025-04-18T09:11:24.708413Z","shell.execute_reply.started":"2025-04-18T09:11:23.794153Z","shell.execute_reply":"2025-04-18T09:11:24.707136Z"},"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[],"execution_count":8},{"cell_type":"markdown","source":"### 4. Vector Database Setup (ChromaDB Initialization and Population)\nThis is where we build the \"memory\" for the similarity check (k-NN). I initialize ChromaDB, telling it to save the database locally in the ./chromadb folder so we don't lose it. I create a \"collection\" (like a table) called fakereviewdb and tell it to use our custom GeminiEmbeddingFunction to generate embeddings. Then, I take all the reviews from the training data (X_train), combine their category, rating, and text into single strings, and add them to the ChromaDB collection. Crucially, for each review added, I also store its true label (Fake/Real) in the metadatas field. This metadata is key for the k-NN prediction step later.","metadata":{}},{"cell_type":"code","source":"import chromadb\n\ngenai.configure(api_key=GOOGLE_API_KEY)\n\n# Initialize ChromaDB client\nchroma_client = chromadb.PersistentClient(path=\"./chromadb\")\nDB_NAME = \"fakereviewdb\"\n\n# Create embedding function instance\nembed_fn = GeminiEmbeddingFunction()\nembed_fn.document_mode = True\n\n# Get or create a collection in ChromaDB\ndb = chroma_client.get_or_create_collection(name=DB_NAME, embedding_function=embed_fn)\n\n# --- Prepare data for adding ---\ndocuments = X_train.apply(lambda row: f\"Category: {row['category']}. Rating: {row['rating']}. Review: {row['text']}\", axis=1).tolist()\nids = [f\"train_{i}\" for i in range(len(documents))]\n\n# *** Create metadata including the labels ***\ntrain_labels = y_train.tolist()\nmetadatas = [{\"label\": label} for label in train_labels]\n\nprint(\"Preparing to re-populate ChromaDB with combined text...\")\n\ntry:\n    chroma_client.delete_collection(name=DB_NAME)\n    print(f\"Collection '{DB_NAME}' deleted.\")\nexcept Exception as e:\n    print(f\"Collection '{DB_NAME}' might not exist, proceeding. Error if any: {e}\")\n\nembed_fn = GeminiEmbeddingFunction()\nembed_fn.document_mode = True\ndb = chroma_client.get_or_create_collection(name=DB_NAME, embedding_function=embed_fn)\n\nprint(f\"Adding/Updating {len(documents)} documents...\")\ndb.add(\n    documents=documents,\n    metadatas=metadatas,\n    ids=ids\n)\nprint(f\"{len(documents)} documents processed.\")\n\nprint(\"Training data embedding complete (including labels as metadata).\")\n\nprint(f\"New collection count: {db.count()}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-18T09:12:37.315515Z","iopub.execute_input":"2025-04-18T09:12:37.315912Z","iopub.status.idle":"2025-04-18T09:28:05.734071Z","shell.execute_reply.started":"2025-04-18T09:12:37.315883Z","shell.execute_reply":"2025-04-18T09:28:05.732824Z"}},"outputs":[{"name":"stdout","text":"Preparing to re-populate ChromaDB with combined text...\nCollection 'fakereviewdb' deleted.\nAdding/Updating 3242 documents...\n3242 documents processed.\nTraining data embedding complete (including labels as metadata).\nNew collection count: 3242\n","output_type":"stream"}],"execution_count":10},{"cell_type":"markdown","source":"### 5. LLM Setup (genai.GenerativeModel, Prompt Template)\nNow setting up the other part of our detection: the stylistic analysis using a powerful language model. I initialize the gemini-1.5-pro model. Then, I define LLM_ANALYSIS_PROMPT_TEMPLATE. This prompt is carefully written to guide the LLM. It specifically asks it to ignore what the review says and focus only on how it's written â€“ looking for tells like weird phrasing, repetition, generic statements, etc., to classify the style as 'Bot' or 'Human'. Asking for a specific output format makes parsing easier.","metadata":{}},{"cell_type":"code","source":"# Setting up the LLM component for style analysis.\nLLM_MODEL_NAME = \"gemini-1.5-pro\"\nllm_model = None\n\ntry:\n    llm_model = genai.GenerativeModel(LLM_MODEL_NAME)\n    print(f\"Initialized LLM: {LLM_MODEL_NAME}\")\nexcept Exception as e:\n    print(f\"ERROR: Failed to initialize LLM '{LLM_MODEL_NAME}'. Check API key/permissions. Linguistic analysis will be skipped. Error: {e}\")\n\n# Prompt: It focuses the LLM on *style*, not content truthfulness.\nLLM_ANALYSIS_PROMPT_TEMPLATE = \"\"\"\nAnalyze the writing style of the following review text to determine if it seems more likely generated by a bot/AI or written by a human.\n\nFocus ONLY on these linguistic patterns:\n- Does it use unnatural phrasing or non-standard sentence structures?\n- Is there excessive repetition of words or phrases?\n- Are there unusual grammar mistakes beyond typical typos?\n- Does it make overly generic statements without specific details?\n- Is the word order strange or syntax awkward?\n- Does it lack personality, sound overly formal, or use fake-sounding excessive enthusiasm?\n\nReview Text (includes category and rating):\n\\\"\\\"\\\"\n{review_text}\n\\\"\\\"\\\"\n\nBased ONLY on the writing style and the patterns above:\n1. Classify the text as either 'Bot' or 'Human'.\n2. Provide a brief explanation (1-2 sentences) for your classification, mentioning specific patterns observed if classified as 'Bot'.\n\nOutput Format:\nClassification: [Bot or Human]\nExplanation: [Your brief explanation]\n\"\"\"","metadata":{"trusted":true,"execution":{"execution_failed":"2025-04-18T15:29:22.119Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### 6. LLM Analysis Function (get_llm_linguistic_assessment)\nThis function orchestrates the call to the LLM. It takes the review text, inserts it into our prompt template, sends it to the initialized llm_model, and gets the response. It then includes logic to parse the LLM's text response to reliably extract the 'Classification' (Bot/Human) and the 'Explanation'. It also handles cases where the LLM might not respond in the exact format expected or if the LLM wasn't initialized correctly, returning \"Error\" or \"Skipped\" status where needed. The @retry.Retry decorator makes the API call more resilient to temporary issues.","metadata":{}},{"cell_type":"code","source":"# Re-use the retry logic if needed for LLM calls\n@retry.Retry(predicate=is_retriable, initial=1.0, maximum=10.0, multiplier=2.0)\ndef get_llm_linguistic_assessment(review_text: str) -> tuple[str, str]:\n    \"\"\"\n    Uses the configured LLM to assess the linguistic style.\n\n    Returns:\n        A tuple: (classification, explanation). Defaults to ('Error', 'LLM analysis failed').\n    \"\"\"\n    if llm_model is None:\n        return \"Skipped\", \"LLM not available\"\n\n    prompt = LLM_ANALYSIS_PROMPT_TEMPLATE.format(review_text=review_text)\n    try:\n        # Adjust generation config as needed (e.g., temperature for creativity)\n        response = llm_model.generate_content(prompt)\n\n        # print(\"-\" * 10 + \" RAW LLM RESPONSE \" + \"-\" * 10)\n        # print(response.text)\n        # print(\"-\" * 30)\n\n        # --- REVISED PARSING LOGIC ---\n        classification = \"Error\"\n        explanation = \"LLM response parsing failed\"\n\n        if response.text:\n            text_response = response.text.strip()\n            lines = text_response.split('\\n')\n            \n            found_classification = False\n            found_explanation = False\n\n            for line in lines:\n                line_stripped = line.strip()\n                if line_stripped.lower().startswith(\"classification:\"):\n                    parts = line_stripped.split(\":\", 1)\n                    if len(parts) > 1:\n                        classification = parts[1].strip().capitalize()\n                        found_classification = True\n                elif line_stripped.lower().startswith(\"explanation:\"):\n                    parts = line_stripped.split(\":\", 1)\n                    if len(parts) > 1:\n                        explanation = parts[1].strip()\n                        found_explanation = True\n\n                # Stop if both found to be slightly more efficient\n                if found_classification and found_explanation:\n                    break\n\n            # If classification was found but explanation wasn't explicitly found\n            if found_classification and not found_explanation:\n                explanation = \"LLM provided classification but no distinct explanation line found.\"\n\n            # If neither was found, use the raw text as explanation for debugging\n            if not found_classification and not found_explanation:\n                explanation = f\"Could not parse classification or explanation from: {text_response[:150]}...\"\n\n        # Validate classification value after parsing\n        if classification not in [\"Bot\", \"Human\"]:  \n            explanation = f\"LLM returned invalid classification value: '{classification}'. Original text: {response.text[:100]}...\"\n            classification = \"Error\"\n\n        return classification, explanation\n\n    except Exception as e:\n        print(f\"ERROR during LLM linguistic analysis for text '{review_text[:50]}...': {e}\")\n        return \"Error\", f\"LLM analysis failed: {e}\"\n\n\nprint(\"LLM analysis function 'get_llm_linguistic_assessment' (with context-aware prompt) defined.\")\n\n#--- Test the LLM function (Optional) ---\ntest_llm_review = \"Syntax strange this product is. Repeat repeat repeat. Good maybe?\"\nllm_class, llm_expl = get_llm_linguistic_assessment(test_llm_review)\nprint(f\"\\nLLM Test - Classification: {llm_class}, Explanation: {llm_expl}\")","metadata":{"trusted":true,"execution":{"execution_failed":"2025-04-18T15:29:22.120Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### 7. k-NN Prediction Function (predict_review_label)\nThis function performs the similarity-based prediction. It takes a new review text, temporarily sets our embedding function to \"query\" mode, and uses db.query to ask ChromaDB for the k==5 most similar reviews from the training data we stored earlier. It then looks at the 'label' stored in the metadata of those neighbors, counts which label ('Fake' or 'Real') appears most often, and returns that majority label as the prediction (1 or 0). The finally block is important to switch the embedding function back to \"document\" mode.","metadata":{}},{"cell_type":"code","source":"K_NEIGHBORS = 5\nprint(f\"Using k = {K_NEIGHBORS} neighbors for k-NN prediction.\")\n\ndef predict_review_label(review_text_combined: str, k: int = K_NEIGHBORS) -> int:\n    \"\"\"\n    Predicts the label for a single combined review text using k-NN search.\n    Returns 0 for Real, 1 for Fake.\n    \"\"\"\n    embed_fn.document_mode = False\n\n    try:\n        results = db.query(\n            query_texts=[review_text_combined],\n            n_results=k,\n            include=['metadatas']\n        )\n\n        if not results or not results.get('ids') or not results['ids'][0]:\n             return 0\n\n        neighbor_metadatas = results['metadatas'][0]\n        neighbor_labels = [metadata.get('label') for metadata in neighbor_metadatas if metadata and 'label' in metadata]\n\n        if not neighbor_labels:\n             return 0\n\n        vote_counts = Counter(neighbor_labels)\n        predicted_label = vote_counts.most_common(1)[0][0]\n        return int(predicted_label)\n\n    except Exception as e:\n        print(f\"ERROR during k-NN prediction for text '{review_text_combined[:50]}...': {e}\")\n        return 0\n    \n        embed_fn.document_mode = True\n\nprint(\"k-NN prediction function 'predict_review_label' defined (operates on combined text).\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-18T09:28:45.100781Z","iopub.execute_input":"2025-04-18T09:28:45.101139Z","iopub.status.idle":"2025-04-18T09:28:45.109789Z","shell.execute_reply.started":"2025-04-18T09:28:45.101112Z","shell.execute_reply":"2025-04-18T09:28:45.108161Z"},"jupyter":{"outputs_hidden":true},"collapsed":true},"outputs":[{"name":"stdout","text":"Using k = 5 neighbors for k-NN prediction.\nk-NN prediction function 'predict_review_label' defined (operates on combined text).\n","output_type":"stream"}],"execution_count":12},{"cell_type":"markdown","source":"### 8. (Optional: Original Agent Function run_fake_review_agent)\nBefore using LangGraph, I had this function to tie everything together. It called the k-NN function, then the LLM function, and finally used some simple if/else logic (basically, if k-NN predicts Fake OR the LLM predicts Bot style, then classify as Fake) to combine the results and create an explanation string. We don't call this directly in the LangGraph version, but its logic is replicated in the 'combiner' node.","metadata":{}},{"cell_type":"code","source":"def run_fake_review_agent(review_text_combined: str, k: int = K_NEIGHBORS) -> dict:\n    \"\"\"\n    Orchestrates k-NN and LLM analysis on combined text.\n    (Logic remains the same as previous Cell 7)\n    Returns:\n        A dictionary containing: 'prediction', 'explanation',\n        'knn_prediction_label', 'llm_assessment', 'llm_explanation'.\n    \"\"\"\n    final_prediction_label = \"Real Review\"\n    explanation = \"\"\n    knn_pred_val = 0\n    llm_assessment = \"Skipped\"\n    llm_explanation = \"N/A\"\n\n    # --- 1. k-NN Prediction ---\n    try:\n        # Passes the combined text to the k-NN function\n        knn_pred_val = predict_review_label(review_text_combined, k=k)\n        knn_label_str = \"Fake\" if knn_pred_val == 1 else \"Real\"\n    except Exception as e:\n        print(f\"ERROR during k-NN prediction step in agent: {e}\")\n        knn_label_str = \"Error\"\n\n    # --- 2. LLM Linguistic Analysis ---\n    try:\n        # Passes the combined text to the LLM function\n        llm_assessment, llm_explanation = get_llm_linguistic_assessment(review_text_combined)\n    except Exception as e:\n        print(f\"ERROR during LLM analysis step in agent: {e}\")\n        llm_assessment = \"Error\"\n        llm_explanation = f\"LLM analysis failed: {e}\"\n\n    # --- 3. Combine Predictions & Generate Explanation ---\n    is_fake_knn = (knn_pred_val == 1)\n    is_bot_llm = (llm_assessment == \"Bot\")\n\n    if is_bot_llm and is_fake_knn:\n        final_prediction_label = \"Fake Review\"\n        explanation = f\"Reasoning: Content similarity suggests fake (k-NN). Writing style also shows bot-like patterns: {llm_explanation}\"\n    elif is_bot_llm:\n        final_prediction_label = \"Fake Review\"\n        explanation = f\"Reasoning: Writing style shows bot-like patterns: {llm_explanation}\"\n    elif is_fake_knn:\n        final_prediction_label = \"Fake Review\"\n        explanation = \"Reasoning: Content similarity suggests fake (k-NN).\"\n        if llm_assessment == \"Human\": explanation += \" Linguistic style appears human.\"\n        elif llm_assessment != \"Real\": explanation += f\" Linguistic style analysis result: {llm_assessment}.\" # Include Skipped/Error\n    else: # Both Real/Human or non-fake errors\n        final_prediction_label = \"Real Review\"\n        explanation = \"Reasoning: Review does not strongly match fake content patterns and linguistic style appears human.\"\n        if knn_label_str == \"Error\" or llm_assessment == \"Error\" or llm_assessment == \"Skipped\":\n             explanation += f\" (Note: k-NN status: {knn_label_str}, LLM status: {llm_assessment}).\"\n\n    return {\n        \"prediction\": final_prediction_label,\n        \"explanation\": explanation,\n        \"knn_prediction_label\": knn_label_str,\n        \"llm_assessment\": llm_assessment,\n        \"llm_explanation\": llm_explanation if is_bot_llm else \"N/A\"\n    }\n\nprint(\"Combined agent logic function 'run_fake_review_agent' defined (operates on combined text).\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-18T09:28:51.909158Z","iopub.execute_input":"2025-04-18T09:28:51.909573Z","iopub.status.idle":"2025-04-18T09:28:51.919820Z","shell.execute_reply.started":"2025-04-18T09:28:51.909539Z","shell.execute_reply":"2025-04-18T09:28:51.918458Z"},"jupyter":{"outputs_hidden":true},"collapsed":true},"outputs":[{"name":"stdout","text":"Combined agent logic function 'run_fake_review_agent' defined (operates on combined text).\n","output_type":"stream"}],"execution_count":13},{"cell_type":"code","source":"!pip install -qU 'langgraph==0.3.21' 'langchain-google-genai==2.1.2' 'langgraph-prebuilt==0.1.7'\n!pip install --upgrade google-generativeai google-api-core chromadb datasets","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-18T09:52:06.955233Z","iopub.execute_input":"2025-04-18T09:52:06.955621Z","iopub.status.idle":"2025-04-18T09:52:21.963470Z","shell.execute_reply.started":"2025-04-18T09:52:06.955592Z","shell.execute_reply":"2025-04-18T09:52:21.961901Z"},"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[{"name":"stdout","text":"Collecting google-generativeai\n  Using cached google_generativeai-0.8.5-py3-none-any.whl.metadata (3.9 kB)\nRequirement already satisfied: google-api-core in /usr/local/lib/python3.10/dist-packages (2.24.2)\nRequirement already satisfied: chromadb in /usr/local/lib/python3.10/dist-packages (0.6.3)\nCollecting chromadb\n  Downloading chromadb-1.0.5-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.9 kB)\nRequirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (3.3.1)\nCollecting datasets\n  Downloading datasets-3.5.0-py3-none-any.whl.metadata (19 kB)\nCollecting google-ai-generativelanguage==0.6.15 (from google-generativeai)\n  Using cached google_ai_generativelanguage-0.6.15-py3-none-any.whl.metadata (5.7 kB)\nRequirement already satisfied: google-api-python-client in /usr/local/lib/python3.10/dist-packages (from google-generativeai) (2.155.0)\nRequirement already satisfied: google-auth>=2.15.0 in /usr/local/lib/python3.10/dist-packages (from google-generativeai) (2.27.0)\nRequirement already satisfied: protobuf in /usr/local/lib/python3.10/dist-packages (from google-generativeai) (5.29.4)\nRequirement already satisfied: pydantic in /usr/local/lib/python3.10/dist-packages (from google-generativeai) (2.11.0a2)\nRequirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from google-generativeai) (4.67.1)\nRequirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from google-generativeai) (4.12.2)\nRequirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in /usr/local/lib/python3.10/dist-packages (from google-ai-generativelanguage==0.6.15->google-generativeai) (1.25.0)\nRequirement already satisfied: googleapis-common-protos<2.0.0,>=1.56.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core) (1.66.0)\nRequirement already satisfied: requests<3.0.0,>=2.18.0 in /usr/local/lib/python3.10/dist-packages (from google-api-core) (2.32.3)\nRequirement already satisfied: build>=1.0.3 in /usr/local/lib/python3.10/dist-packages (from chromadb) (1.2.2.post1)\nRequirement already satisfied: chroma-hnswlib==0.7.6 in /usr/local/lib/python3.10/dist-packages (from chromadb) (0.7.6)\nCollecting fastapi==0.115.9 (from chromadb)\n  Downloading fastapi-0.115.9-py3-none-any.whl.metadata (27 kB)\nRequirement already satisfied: uvicorn>=0.18.3 in /usr/local/lib/python3.10/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.34.1)\nRequirement already satisfied: numpy>=1.22.5 in /usr/local/lib/python3.10/dist-packages (from chromadb) (1.26.4)\nRequirement already satisfied: posthog>=2.4.0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (3.25.0)\nRequirement already satisfied: onnxruntime>=1.14.1 in /usr/local/lib/python3.10/dist-packages (from chromadb) (1.21.0)\nRequirement already satisfied: opentelemetry-api>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (1.32.1)\nRequirement already satisfied: opentelemetry-exporter-otlp-proto-grpc>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (1.32.1)\nRequirement already satisfied: opentelemetry-instrumentation-fastapi>=0.41b0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (0.53b1)\nRequirement already satisfied: opentelemetry-sdk>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (1.32.1)\nRequirement already satisfied: tokenizers>=0.13.2 in /usr/local/lib/python3.10/dist-packages (from chromadb) (0.21.0)\nRequirement already satisfied: pypika>=0.48.9 in /usr/local/lib/python3.10/dist-packages (from chromadb) (0.48.9)\nRequirement already satisfied: overrides>=7.3.1 in /usr/local/lib/python3.10/dist-packages (from chromadb) (7.7.0)\nRequirement already satisfied: importlib-resources in /usr/local/lib/python3.10/dist-packages (from chromadb) (5.13.0)\nRequirement already satisfied: grpcio>=1.58.0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (1.68.1)\nRequirement already satisfied: bcrypt>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from chromadb) (4.3.0)\nRequirement already satisfied: typer>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (0.15.1)\nRequirement already satisfied: kubernetes>=28.1.0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (32.0.1)\nRequirement already satisfied: tenacity>=8.2.3 in /usr/local/lib/python3.10/dist-packages (from chromadb) (9.0.0)\nRequirement already satisfied: pyyaml>=6.0.0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (6.0.2)\nRequirement already satisfied: mmh3>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from chromadb) (5.1.0)\nRequirement already satisfied: orjson>=3.9.12 in /usr/local/lib/python3.10/dist-packages (from chromadb) (3.10.12)\nRequirement already satisfied: httpx>=0.27.0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (0.28.1)\nRequirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (13.9.4)\nRequirement already satisfied: jsonschema>=4.19.0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (4.23.0)\nCollecting starlette<0.46.0,>=0.40.0 (from fastapi==0.115.9->chromadb)\n  Downloading starlette-0.45.3-py3-none-any.whl.metadata (6.3 kB)\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.17.0)\nRequirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (19.0.1)\nRequirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.8)\nRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.2.3)\nRequirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.5.0)\nRequirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.16)\nRequirement already satisfied: fsspec<=2024.12.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets) (2024.12.0)\nRequirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.11.12)\nRequirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.29.0)\nRequirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (24.2)\nRequirement already satisfied: pyproject_hooks in /usr/local/lib/python3.10/dist-packages (from build>=1.0.3->chromadb) (1.2.0)\nRequirement already satisfied: tomli>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from build>=1.0.3->chromadb) (2.2.1)\nRequirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.4.6)\nRequirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.2)\nRequirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (5.0.1)\nRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (25.1.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.5.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.1.0)\nRequirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (0.2.1)\nRequirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.18.3)\nRequirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth>=2.15.0->google-generativeai) (5.5.0)\nRequirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth>=2.15.0->google-generativeai) (0.4.1)\nRequirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth>=2.15.0->google-generativeai) (4.9)\nRequirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx>=0.27.0->chromadb) (4.9.0)\nRequirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx>=0.27.0->chromadb) (2025.1.31)\nRequirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx>=0.27.0->chromadb) (1.0.7)\nRequirement already satisfied: idna in /usr/local/lib/python3.10/dist-packages (from httpx>=0.27.0->chromadb) (3.10)\nRequirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx>=0.27.0->chromadb) (0.14.0)\nRequirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=4.19.0->chromadb) (2024.10.1)\nRequirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=4.19.0->chromadb) (0.35.1)\nRequirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=4.19.0->chromadb) (0.22.3)\nRequirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb) (1.17.0)\nRequirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb) (2.9.0.post0)\nRequirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb) (1.8.0)\nRequirement already satisfied: requests-oauthlib in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb) (1.3.1)\nRequirement already satisfied: oauthlib>=3.2.2 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb) (3.2.2)\nRequirement already satisfied: urllib3>=1.24.2 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb) (2.3.0)\nRequirement already satisfied: durationpy>=0.7 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb) (0.9)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy>=1.22.5->chromadb) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy>=1.22.5->chromadb) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy>=1.22.5->chromadb) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy>=1.22.5->chromadb) (2025.0.1)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy>=1.22.5->chromadb) (2022.0.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy>=1.22.5->chromadb) (2.4.1)\nRequirement already satisfied: coloredlogs in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.14.1->chromadb) (15.0.1)\nRequirement already satisfied: flatbuffers in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.14.1->chromadb) (24.3.25)\nRequirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.14.1->chromadb) (1.13.1)\nRequirement already satisfied: deprecated>=1.2.6 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-api>=1.2.0->chromadb) (1.2.15)\nRequirement already satisfied: importlib-metadata<8.7.0,>=6.0 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-api>=1.2.0->chromadb) (8.5.0)\nRequirement already satisfied: opentelemetry-exporter-otlp-proto-common==1.32.1 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.32.1)\nRequirement already satisfied: opentelemetry-proto==1.32.1 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.32.1)\nRequirement already satisfied: opentelemetry-instrumentation-asgi==0.53b1 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (0.53b1)\nRequirement already satisfied: opentelemetry-instrumentation==0.53b1 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (0.53b1)\nRequirement already satisfied: opentelemetry-semantic-conventions==0.53b1 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (0.53b1)\nRequirement already satisfied: opentelemetry-util-http==0.53b1 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (0.53b1)\nRequirement already satisfied: wrapt<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-instrumentation==0.53b1->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (1.17.0)\nRequirement already satisfied: asgiref~=3.0 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-instrumentation-asgi==0.53b1->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (3.8.1)\nRequirement already satisfied: monotonic>=1.5 in /usr/local/lib/python3.10/dist-packages (from posthog>=2.4.0->chromadb) (1.6)\nRequirement already satisfied: backoff>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from posthog>=2.4.0->chromadb) (2.2.1)\nRequirement already satisfied: distro>=1.5.0 in /usr/local/lib/python3.10/dist-packages (from posthog>=2.4.0->chromadb) (1.9.0)\nRequirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic->google-generativeai) (0.7.0)\nRequirement already satisfied: pydantic-core==2.29.0 in /usr/local/lib/python3.10/dist-packages (from pydantic->google-generativeai) (2.29.0)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.18.0->google-api-core) (3.4.1)\nRequirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->chromadb) (3.0.0)\nRequirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->chromadb) (2.19.1)\nRequirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from typer>=0.9.0->chromadb) (8.1.7)\nRequirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer>=0.9.0->chromadb) (1.5.4)\nRequirement already satisfied: httptools>=0.6.3 in /usr/local/lib/python3.10/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.6.4)\nRequirement already satisfied: python-dotenv>=0.13 in /usr/local/lib/python3.10/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (1.1.0)\nRequirement already satisfied: uvloop!=0.15.0,!=0.15.1,>=0.14.0 in /usr/local/lib/python3.10/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.21.0)\nRequirement already satisfied: watchfiles>=0.13 in /usr/local/lib/python3.10/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (1.0.5)\nRequirement already satisfied: websockets>=10.4 in /usr/local/lib/python3.10/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (14.1)\nRequirement already satisfied: httplib2<1.dev0,>=0.19.0 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client->google-generativeai) (0.22.0)\nRequirement already satisfied: google-auth-httplib2<1.0.0,>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client->google-generativeai) (0.2.0)\nRequirement already satisfied: uritemplate<5,>=3.0.1 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client->google-generativeai) (4.1.1)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2025.1)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2025.1)\nRequirement already satisfied: grpcio-status<2.0.dev0,>=1.33.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.15->google-generativeai) (1.48.2)\nRequirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in /usr/local/lib/python3.10/dist-packages (from httplib2<1.dev0,>=0.19.0->google-api-python-client->google-generativeai) (3.2.0)\nRequirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.10/dist-packages (from importlib-metadata<8.7.0,>=6.0->opentelemetry-api>=1.2.0->chromadb) (3.21.0)\nRequirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->chromadb) (0.1.2)\nRequirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=2.15.0->google-generativeai) (0.6.1)\nRequirement already satisfied: exceptiongroup>=1.0.2 in /usr/local/lib/python3.10/dist-packages (from anyio->httpx>=0.27.0->chromadb) (1.2.2)\nRequirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio->httpx>=0.27.0->chromadb) (1.3.1)\nRequirement already satisfied: humanfriendly>=9.1 in /usr/local/lib/python3.10/dist-packages (from coloredlogs->onnxruntime>=1.14.1->chromadb) (10.0)\nRequirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.22.5->chromadb) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.22.5->chromadb) (2022.0.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy>=1.22.5->chromadb) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy>=1.22.5->chromadb) (2024.2.0)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->onnxruntime>=1.14.1->chromadb) (1.3.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy>=1.22.5->chromadb) (2024.2.0)\nUsing cached google_generativeai-0.8.5-py3-none-any.whl (155 kB)\nUsing cached google_ai_generativelanguage-0.6.15-py3-none-any.whl (1.3 MB)\nDownloading chromadb-1.0.5-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.3 MB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m18.3/18.3 MB\u001b[0m \u001b[31m67.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading fastapi-0.115.9-py3-none-any.whl (94 kB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m94.9/94.9 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading datasets-3.5.0-py3-none-any.whl (491 kB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m491.2/491.2 kB\u001b[0m \u001b[31m20.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading starlette-0.45.3-py3-none-any.whl (71 kB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m71.5/71.5 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: starlette, fastapi, google-ai-generativelanguage, google-generativeai, datasets, chromadb\n  Attempting uninstall: starlette\n    Found existing installation: starlette 0.46.2\n    Uninstalling starlette-0.46.2:\n      Successfully uninstalled starlette-0.46.2\n  Attempting uninstall: fastapi\n    Found existing installation: fastapi 0.115.12\n    Uninstalling fastapi-0.115.12:\n      Successfully uninstalled fastapi-0.115.12\n  Attempting uninstall: google-ai-generativelanguage\n    Found existing installation: google-ai-generativelanguage 0.6.17\n    Uninstalling google-ai-generativelanguage-0.6.17:\n      Successfully uninstalled google-ai-generativelanguage-0.6.17\n  Attempting uninstall: datasets\n    Found existing installation: datasets 3.3.1\n    Uninstalling datasets-3.3.1:\n      Successfully uninstalled datasets-3.3.1\n  Attempting uninstall: chromadb\n    Found existing installation: chromadb 0.6.3\n    Uninstalling chromadb-0.6.3:\n      Successfully uninstalled chromadb-0.6.3\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nlangchain-google-genai 2.1.2 requires google-ai-generativelanguage<0.7.0,>=0.6.16, but you have google-ai-generativelanguage 0.6.15 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed chromadb-1.0.5 datasets-3.5.0 fastapi-0.115.9 google-ai-generativelanguage-0.6.15 google-generativeai-0.8.5 starlette-0.45.3\n","output_type":"stream"}],"execution_count":18},{"cell_type":"markdown","source":"### 9. LangGraph Agent Implementation (State, Nodes, Graph)\nThis is where we structure the detection process using LangGraph.\n- **ReviewAgentState:** I define the \"memory\" or state for our agent. It holds the input review and slots for the results from each step (k-NN label, LLM assessment, etc.) and the final prediction/explanation.\n- **Nodes (knn_predictor_node, etc.):** I define Python functions that represent each distinct step in our process. Each node function takes the current state, performs its task (like calling predict_review_label or get_llm_linguistic_assessment), and returns a dictionary containing only the updates to the state.\n- **Graph (StateGraph, add_node, add_edge):** I build the workflow. I create a StateGraph, add our nodes to it, define the starting point (knn_predictor), and then specify the sequence using add_edge. In this case, it's a simple linear flow: k-NN -> LLM -> Combiner -> END.\n- **compile():** This turns the graph definition into a runnable agent object.","metadata":{}},{"cell_type":"code","source":"import operator\nfrom typing import TypedDict, Annotated, Optional, Sequence\nfrom langgraph.graph import StateGraph, END\nfrom langchain_core.messages import BaseMessage\nfrom collections import Counter\nimport json \n\n# 1. Define the State for the Agent Graph\nclass ReviewAgentState(TypedDict):\n    \"\"\"Represents the state of our fake review detection agent.\"\"\"\n    review_text: str\n\n    # Intermediate results\n    knn_prediction_label: Optional[int]\n    knn_label_str: Optional[str]\n    llm_assessment: Optional[str]\n    llm_explanation: Optional[str]\n\n    # Final output\n    final_prediction: Optional[str]\n    final_explanation: Optional[str]\n\n    # Error handling\n    error_message: Optional[str]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-18T10:11:20.891282Z","iopub.execute_input":"2025-04-18T10:11:20.891670Z","iopub.status.idle":"2025-04-18T10:11:21.345481Z","shell.execute_reply.started":"2025-04-18T10:11:20.891641Z","shell.execute_reply":"2025-04-18T10:11:21.344102Z"}},"outputs":[],"execution_count":19},{"cell_type":"code","source":"# 2. Define the Nodes (functions that operate on the state)\n\ndef knn_predictor_node(state: ReviewAgentState) -> dict:\n    \"\"\"Runs the k-NN prediction and updates the state.\"\"\"\n    print(\"--- Running k-NN Predictor Node ---\")\n    review_text = state.get(\"review_text\")\n    if not review_text:\n        print(\"ERROR: No review text provided to k-NN node.\")\n        return {\"error_message\": \"No review text provided to k-NN node.\", \"knn_label_str\": \"Error\"}\n\n    try:\n        knn_pred_val = predict_review_label(review_text, k=K_NEIGHBORS)\n        knn_label_str = \"Fake\" if knn_pred_val == 1 else \"Real\"\n        print(f\"k-NN Prediction: {knn_label_str} ({knn_pred_val})\")\n        return {\n            \"knn_prediction_label\": knn_pred_val,\n            \"knn_label_str\": knn_label_str,\n            \"error_message\": None\n        }\n    except Exception as e:\n        print(f\"ERROR in k-NN Node: {e}\")\n        return {\n            \"knn_prediction_label\": None,\n            \"knn_label_str\": \"Error\",\n            \"error_message\": f\"k-NN prediction failed: {e}\"\n        }\n\ndef llm_analyzer_node(state: ReviewAgentState) -> dict:\n    \"\"\"Runs the LLM linguistic analysis and updates the state.\"\"\"\n    print(\"--- Running LLM Analyzer Node ---\")\n    review_text = state.get(\"review_text\")\n    if not review_text:\n        print(\"ERROR: No review text provided to LLM node.\")\n        return {\"error_message\": \"No review text provided to LLM node.\", \"llm_assessment\": \"Error\", \"llm_explanation\": \"Missing input\"}\n\n    if llm_model is None:\n         print(\"LLM model not available, skipping analysis.\")\n         return {\n             \"llm_assessment\": \"Skipped\",\n             \"llm_explanation\": \"LLM not initialized\",\n             \"error_message\": state.get(\"error_message\")}\n\n    try:\n        llm_assessment, llm_explanation = get_llm_linguistic_assessment(review_text)\n        print(f\"LLM Assessment: {llm_assessment}\")\n        # Only store LLM explanation if it's relevant (e.g., for 'Bot' or 'Error')\n        explanation_to_store = llm_explanation if llm_assessment in [\"Bot\", \"Error\"] else \"N/A\"\n\n        # Determine if this step introduced an error\n        current_error = f\"LLM analysis failed: {llm_explanation}\" if llm_assessment == \"Error\" else None\n        previous_error = state.get(\"error_message\")\n        combined_error = previous_error if previous_error and not current_error else current_error # Prioritize new error, otherwise keep old\n\n        return {\n            \"llm_assessment\": llm_assessment,\n            \"llm_explanation\": explanation_to_store,\n            \"error_message\": combined_error}\n        \n    except Exception as e:\n        print(f\"ERROR in LLM Node: {e}\")\n        # Combine with potential previous error\n        previous_error = state.get(\"error_message\")\n        current_error = f\"LLM analysis failed: {e}\"\n        combined_error = f\"{previous_error}; {current_error}\" if previous_error else current_error\n        return {\n            \"llm_assessment\": \"Error\",\n            \"llm_explanation\": f\"LLM analysis failed: {e}\",\n            \"error_message\": combined_error\n        }\n\ndef combiner_node(state: ReviewAgentState) -> dict:\n    \"\"\"Combines k-NN and LLM results to produce the final output.\"\"\"\n    print(\"--- Running Combiner Node ---\")\n    knn_pred_val = state.get(\"knn_prediction_label\")\n    knn_label_str = state.get(\"knn_label_str\", \"Error\")\n    llm_assessment = state.get(\"llm_assessment\", \"Skipped\")\n    llm_explanation = state.get(\"llm_explanation\", \"N/A\")\n\n    final_prediction_label = \"Real Review\" # Default\n    explanation = \"\"\n\n    # Handle cases where knn_pred_val might be None due to error\n    is_fake_knn = (knn_pred_val == 1) if knn_pred_val is not None else False\n    is_bot_llm = (llm_assessment == \"Bot\")\n\n    # Replicate the combination logic from run_fake_review_agent\n    if is_bot_llm and is_fake_knn:\n        final_prediction_label = \"Fake Review\"\n        # Use the stored explanation which is only present for Bot/Error\n        explanation = f\"Reasoning: Content similarity suggests fake (k-NN: {knn_label_str}). Writing style also shows bot-like patterns: {llm_explanation}\"\n    elif is_bot_llm:\n        final_prediction_label = \"Fake Review\"\n        explanation = f\"Reasoning: Writing style shows bot-like patterns: {llm_explanation}\"\n        if knn_label_str != \"Real\": explanation += f\" (k-NN status: {knn_label_str}).\"\n    elif is_fake_knn:\n        final_prediction_label = \"Fake Review\"\n        explanation = f\"Reasoning: Content similarity suggests fake (k-NN: {knn_label_str}).\"\n        if llm_assessment == \"Human\": explanation += \" Linguistic style appears human.\"\n        # Check llm_assessment against \"Human\" or \"Real\" (assuming Real is not a valid LLM output here)\n        elif llm_assessment != \"Human\": explanation += f\" Linguistic style analysis result: {llm_assessment}.\"\n    else: # Both Real/Human or non-fake errors/skipped\n        final_prediction_label = \"Real Review\"\n        explanation = \"Reasoning: Review does not strongly match fake content patterns based on available analysis.\"\n        # Add more detail if components were skipped or erred\n        status_notes = []\n        if knn_label_str not in [\"Real\", \"Fake\"]: status_notes.append(f\"k-NN status: {knn_label_str}\") # e.g., Error\n        if llm_assessment not in [\"Human\", \"Bot\"]: status_notes.append(f\"LLM status: {llm_assessment}\") # e.g., Skipped, Error\n        if status_notes:\n            explanation += f\" (Note: {', '.join(status_notes)}).\"\n        elif llm_assessment == \"Human\" and knn_label_str == \"Real\":\n             explanation = \"Reasoning: Review does not strongly match fake content patterns (k-NN: Real) and linguistic style appears human.\"\n\n    print(f\"Final Prediction: {final_prediction_label}\")\n    # Return only the final fields updated by this node\n    return {\n        \"final_prediction\": final_prediction_label,\n        \"final_explanation\": explanation\n    }\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-18T10:19:38.715523Z","iopub.execute_input":"2025-04-18T10:19:38.716131Z","iopub.status.idle":"2025-04-18T10:19:38.731309Z","shell.execute_reply.started":"2025-04-18T10:19:38.716081Z","shell.execute_reply":"2025-04-18T10:19:38.730004Z"}},"outputs":[],"execution_count":20},{"cell_type":"code","source":"# 3. Define the Graph just to show the workflow\nworkflow = StateGraph(ReviewAgentState)\n\n# Add the nodes\nworkflow.add_node(\"knn_predictor\", knn_predictor_node)\nworkflow.add_node(\"llm_analyzer\", llm_analyzer_node)\nworkflow.add_node(\"combiner\", combiner_node)\n\n# Set the entrypoint\nworkflow.set_entry_point(\"knn_predictor\")\n\n# Add edges for the sequential flow\nworkflow.add_edge(\"knn_predictor\", \"llm_analyzer\")\nworkflow.add_edge(\"llm_analyzer\", \"combiner\")\nworkflow.add_edge(\"combiner\", END) # End the graph after combining\n\n# Compile the graph\nagent_graph = workflow.compile()\n\ntry:\n    from IPython.display import Image, display\n    display(Image(agent_graph.get_graph().draw_mermaid_png()))\nexcept Exception as e:\n    print(f\"Could not draw graph: {e}. Ensure graphviz and pygraphviz are installed.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-18T10:20:39.764806Z","iopub.execute_input":"2025-04-18T10:20:39.765344Z","iopub.status.idle":"2025-04-18T10:20:40.481350Z","shell.execute_reply.started":"2025-04-18T10:20:39.765283Z","shell.execute_reply":"2025-04-18T10:20:40.479972Z"}},"outputs":[{"output_type":"display_data","data":{"image/png":"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\n","text/plain":"<IPython.core.display.Image object>"},"metadata":{}}],"execution_count":21},{"cell_type":"markdown","source":"### 10. Running the LangGraph Agent (Grand Final)\nShowtime! I take a review from the test set, format it, and create the initial_state dictionary containing just this input review. Then, I call agent_graph.invoke(initial_state). LangGraph takes care of passing the state through each node in the sequence we defined (k-NN, then LLM, then Combiner). The final output (final_state) contains the accumulated results from all steps. I then print out a summary comparing the agent's prediction to the actual known label for that test review.\n\nThe example below shows that when kNN predicted \"Real\", the LLM was able to accurately predict it was a bot created review based on the language nuance. I perseonally find this amazing and gives many ways to improve your traditional rule based models with the complimentation of LLMs.","metadata":{}},{"cell_type":"code","source":"# --- Running the Agent ---\nprint(\"\\n--- Testing the LangGraph Agent ---\")\n\n# Example input reviews (using the combined format)\nfirst_row = X_test.iloc[0] # Get the first row as a Series\ntest_review_real = f\"Category: {first_row['category']}. Rating: {first_row['rating']}. Review: {first_row['text']}\"# Get first test review\ntest_review_fake_style = f\"Category: Electronics. Rating: 1. Review: Product bad is. Not function good. Repeat repeat repeat. Unnatural syntax this review has. Do not buy this thing ever.\"\ntest_review_fake_knn = f\"Category: Home_and_Kitchen. Rating: 5. Review: Absolutely amazing product! Changed my life. So versatile and easy to use. Everyone should own one. Best purchase ever!\" # Generic, might match other fake ones\n\n# --- Select a review to test ---\n# Combine details from the first row of X_test\ntest_index = 10 # Choose a row index from X_test\ntest_row = X_test.iloc[test_index] # Get the row as a Series\ninput_category = X_test.iloc[test_index]['category']\ninput_rating = X_test.iloc[test_index]['rating']\ninput_text = X_test.iloc[test_index]['text']\nactual_label_numeric = y_test.iloc[test_index] # Get the corresponding label\nactual_label_str = \"Fake\" if actual_label_numeric == 1 else \"Real\"\n\ninput_review_combined = f\"Category: {test_row['category']}. Rating: {test_row['rating']}. Review: {test_row['text']}\"\n\nprint(f\"Input Review (Index {test_index}, Actual Label: {actual_label_str}):\\n{input_review_combined}\\n\")\n\n# --- Prepare the initial state ---\ninitial_state = {\"review_text\": input_review_combined}\n\n# Invoke the graph\nconfig = {\"recursion_limit\": 5}\nfinal_state = agent_graph.invoke(initial_state, config=config)\n\n# print(\"\\n--- Final Agent State (Full) ---\")\n# Pretty print the final state dictionary\n# print(json.dumps(final_state, indent=2))\n\nprint(\"\\n--- Agent Result Summary ---\")\nprint(f\"Input Review: {final_state.get('review_text')[:100]}...\")\nprint(f\"Actual Label: {actual_label_str}\")\nprint(f\"k-NN Result: {final_state.get('knn_label_str')}\")\nprint(f\"LLM Assessment: {final_state.get('llm_assessment')}\")\nprint(f\"----> Final Prediction: {final_state.get('final_prediction')}\")\nprint(f\"Explanation: {final_state.get('final_explanation')}\")\nif final_state.get('error_message'):\n    print(f\"Error Message: {final_state.get('error_message')}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-18T10:39:10.052054Z","iopub.execute_input":"2025-04-18T10:39:10.052625Z","iopub.status.idle":"2025-04-18T10:39:11.850973Z","shell.execute_reply.started":"2025-04-18T10:39:10.052583Z","shell.execute_reply":"2025-04-18T10:39:11.849241Z"}},"outputs":[{"name":"stdout","text":"\n--- Testing the LangGraph Agent ---\nInput Review (Index 10, Actual Label: Fake):\nCategory: Sports_and_Outdoors. Rating: 5.0. Review: Great little gadget to use for an emergency. Not too big or too small.\n\n--- Running k-NN Predictor Node ---\nk-NN Prediction: Real (0)\n--- Running LLM Analyzer Node ---\nLLM Assessment: Bot\n--- Running Combiner Node ---\nFinal Prediction: Fake Review\n\n--- Agent Result Summary ---\nInput Review: Category: Sports_and_Outdoors. Rating: 5.0. Review: Great little gadget to use for an emergency. Not...\nActual Label: Fake\nk-NN Result: Real\nLLM Assessment: Bot\n----> Final Prediction: Fake Review\nExplanation: Reasoning: Writing style shows bot-like patterns: The review uses generic phrasing (\"Great little gadget,\" \"Not too big or too small\") without specific details about the product's usefulness or functionality, suggesting automated generation. The language is also simplistic and lacks personality.\n","output_type":"stream"}],"execution_count":26},{"cell_type":"markdown","source":"## Challenges Overcame\n\n- **Getting the Core Components Right:** Initially, there were hurdles ensuring the custom GeminiEmbeddingFunction correctly interacted with the latest google-generativeai library (like the embed_content API and task_type usage). Google-genai was keep giving error when traying to use embed_content(). I also refined the k-NN prediction function (predict_review_label) to reliably switch the embedding function between 'query' and 'document' modes using a finally block, preventing potential issues later on.\n  \n- **Structuring with LangGraph:** Translating the sequential logic (run k-NN, run LLM, combine) into the LangGraph framework was the main structural task. This involved defining a clear State dictionary (ReviewAgentState) to hold and pass information between steps, and then wrapping my existing k-NN and LLM functions within specific 'node' functions designed to update this state correctly (returning dictionaries, not just values). Understanding how nodes access shared resources like embed_fn and db was also key, for this notebook, using the global scope worked best.\n\n- **Debugging Runtime Errors:** A significant runtime challenge popped up as a TypeError when preparing test data right before invoking the agent. The traceback helped pinpoint that I was incorrectly using pandas .apply() on a single row (a Series) instead of directly accessing its elements; correcting this data access method resolved the error and allowed the agent to run successfully.\n","metadata":{}},{"cell_type":"markdown","source":"### Next Steps\n\nDue to time limitations and knowledge that I learned from the course, I am presenting here the initial version of my Agent. In future, I would like to keep improving the Agent with following capabilities:\n\n- Integrate:\n  - User Behavior Analysis\n  - Review Velocity/Timing\n  - Review Network Analysis\n  - Sentiment Analysis\n  - Image Analysis\n- Fine-tune smaller models\n- Add feedback loop\n- Weighted Voting: Instead of a simple majority vote, weight neighbor votes by their distance/similarity score (closer neighbors get a stronger vote).\n\nWhen cost and regular maintainance is not an issue, the Agent can compare the review with general sentiment, common themes, or even stylistic patterns observed in reviews for similar products on external platforms and make decision. It requires adding data retrieval tools and analysis nodes, then updating the workflow and combination logic to leverage this new information source.","metadata":{}}]}